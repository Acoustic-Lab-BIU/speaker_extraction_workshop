{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoRAIM winter school - Speaker Extraction Workshop \n",
    "#### Hosted by Bar Ilan University (BIU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intruduction (10 min)\n",
    "<img src=\"media/ros4hri_ids.png\" alt=\"h\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech processing in HRI\n",
    "\n",
    "In the SPRING project the patients and ARI are aving a verbal dialuge. therefore the ability to understand the patient is key.\n",
    "When failing to understand the patient, ARI could response poorly regarding the exeptence of ARI or just respond in a complitly wrong matter.\n",
    "Scence the patient's speech might be overlaped with another human's voice (doctors,accompanying person,backround speech etc.) we need to encounter and somhow get the wanted speech only.\n",
    "\n",
    "\n",
    "There are two main ways of doing such task:\n",
    " 1. \"Blind\" source separation - splits the overlaping speech to two channel (one for each speaker) with no prior data about the speakers.\n",
    " 2. Speaker extraction - extract the wanted speaker from the overlaping speech by a refrence audio signal of the wanted speaker.\n",
    "\n",
    "Today we will focus on speaker extraction.\n",
    "\n",
    "# <span style=\"color:red\"> **example audio and ASR ** </span>\n",
    "\n",
    "\n",
    "  \n",
    "## The tasks for today's workshop:\n",
    "- [] Basics speaker extraction.\n",
    "- [] Audio recording and feature extraction of audio data.\n",
    "- [] Extracting your own voice from a mixture of two voices.\n",
    "- [] Evaluating your results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basics of Speaker Extraction (20 minutes)\n",
    "\n",
    "#### Target speech/speaker extraction (TSE) isolates the speech signal of a target speaker from a mixture of several speakers, with or without noises and  everberations, using clues that identify the speaker in the mixture. Such clues might be a spatial clue indicating the direction of the target speaker, a video of the speaker’s lips, and a prerecorded enrollment utterance from which the speaker’s voice characteristics can be derived. \n",
    "<img src=\"media/delcroix01-3240008-large.gif\" alt=\"extraction_clues\" width=\"700\">\n",
    "\n",
    "- K. Zmolikova, M. Delcroix, T. Ochiai, K. Kinoshita, J. Cernocký, and D. Yu, ‘‘Neural target speech extraction: An overview,’’ IEEE Signal Process. Mag., vol. 0,  o. 3, pp. 8–29, May 2023 \n",
    "  \n",
    "\n",
    "#### In our approach, we propose a Siamese-Unet architecture that uses both representations. The Siamese encoders are applied in the frequency domain to infer the embedding of the noisy and reference spectra, respectively. The concatenated representations are then fed into the decoder to estimate the real and imaginary  omponents of the desired speaker, which are then inverse-transformed to the time-domain.\n",
    "\n",
    "- A. Eisenberg, S. Gannot and S. E. Chazan, \"Single microphone speaker extraction using unified time-frequency Siamese-Unet,\" 2022 30th European Signal Processing Conference (EUSIPCO), Belgrade, Serbia, 2022, pp. 762-766, doi: 10.23919/EUSIPCO55093.2022.9909545. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction to the process of recording speech samples on laptops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Provide instructions for participants to record their own speech samples on their laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hands-on Session: Recording Speech Samples (10 minutes)\n",
    "\n",
    "- Allow time for participants to record their speech samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-on Session: Extracting Voices (30 minutes)\n",
    "- Provide participants with sample code or tools for voice extraction on their laptops (show an example,explain the class methods).\n",
    "- Guide participants through the process of extracting voices from their recorded speech\n",
    "samples using the provided model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Discussion (10 minutes)\n",
    "- Encourage participants to evaluate the eﬀectiveness of their voice extraction.(sisdr ,asr if possible)\n",
    "- maybe ask for a volenteer to present his good/bad extraction.\n",
    "- Discuss potential applications of voice extraction in HRI research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion (10 minutes)\n",
    "- Summarize key takeaways from the workshop, emphasizing their relevance to the SPRING\n",
    "project and its goals.\n",
    "- Provide additional resources for further learning. (papers, sharon's website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
